import json
import time
from typing import Optional, List
import aiohttp
import asyncio
import os
from openai import OpenAI

from config.env import AppConfig
from exceptions.translate_exceptions import LLMAPIException
from utils.log_util import logger


class LLMClient:
    def __init__(self, api_key: str, base_url: str = "https://api.llvm.org/"):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL

        Raises:
            ValueError: APIå¯†é’¥æœªè®¾ç½®æ—¶
        """
        # ä¼˜å…ˆçº§: ä¼ å…¥å‚æ•° > ç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶
        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY") or AppConfig.dashscope_api_key

        if not self.api_key:
            raise ValueError("DASHSCOPE_API_KEY must be set in environment or config")

        self.base_url = base_url.rstrip('/')
        self.session: Optional[aiohttp.ClientSession] = None
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def translate_batch(self, texts: List[str], user_level: str, timeout: int = 30) -> List[str]:
        """
        æ‰¹é‡ç¿»è¯‘æ–‡æœ¬ï¼ˆå¸¦é‡è¯•å’Œè¶…æ—¶ï¼‰

        Args:
            texts: æ–‡æœ¬æ•°ç»„
            user_level: ç”¨æˆ·ç­‰çº§
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            ç¿»è¯‘åçš„æ–‡æœ¬åˆ—è¡¨

        Raises:
            LLMAPIException: LLM API è°ƒç”¨å¤±è´¥
        """
        start_time = time.time()

        # âœ… è®°å½• API è°ƒç”¨å¼€å§‹
        logger.debug("Calling LLM API", extra={
            "model": "qwen-plus",
            "text_count": len(texts),
            "user_level": user_level
        })

        try:
            my_prompts = f"""
ä½ æ˜¯ä¸€ä¸ªä¸­è‹±æ··åˆå­¦ä¹ åŠ©æ‰‹ï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·çš„è‹±è¯­æ°´å¹³ï¼Œå°†ä¸­æ–‡æ–‡æœ¬ä¸­çš„éƒ¨åˆ†è¯æ±‡æ›¿æ¢ä¸ºåˆé€‚çš„è‹±æ–‡å•è¯ï¼Œå¸®åŠ©ç”¨æˆ·åœ¨é˜…è¯»ä¸­è‡ªç„¶ä¹ å¾—æ–°è¯æ±‡ã€‚

**å¤„ç†è§„åˆ™ï¼š**
1. **ç”¨æˆ·æ°´å¹³åˆ¤æ–­**ï¼šç”¨æˆ·å½“å‰è‹±è¯­ç­‰çº§ä¸ºã€{user_level}ã€‘ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†é€‰æ‹©æ›¿æ¢è¯æ±‡ï¼š
   - åˆçº§ï¼šæ›¿æ¢æœ€åŸºç¡€çš„æ—¥å¸¸è¯æ±‡ï¼ˆå¦‚åè¯ã€ç®€å•åŠ¨è¯ï¼‰
   - ä¸­çº§ï¼šæ›¿æ¢å¸¸è§å½¢å®¹è¯ã€çŸ­è¯­åŠ¨è¯ã€å¸¸ç”¨è¡¨è¾¾
   - é«˜çº§ï¼šæ›¿æ¢è¾ƒä¸“ä¸šçš„æœ¯è¯­ã€å­¦æœ¯è¯æ±‡ã€æƒ¯ç”¨è¯­
2. **æ›¿æ¢åŸåˆ™**ï¼š
   - åªæ›¿æ¢èƒ½ä¿ƒè¿›å­¦ä¹ çš„"å¯ä¹ å¾—è¯æ±‡"ï¼ˆç¬¦åˆi+1è¾“å…¥å‡è¯´ï¼‰
   - ä¿æŒåŸå¥ç»“æ„å’Œè¯­æ³•å®Œæ•´
   - ä¸æ›¿æ¢ä¸“æœ‰åè¯ã€äººåã€å“ç‰Œå
   - ä¸æ›¿æ¢æ•°å­—ã€æ—¥æœŸã€åº¦é‡å•ä½
   - ä¸æ”¹å˜UIå…ƒç´ ã€æŒ‰é’®åç§°ã€ç•Œé¢æ–‡æ¡ˆ
3. **è¾“å‡ºæ ¼å¼**ï¼šè¿”å›ä¸è¾“å…¥å®Œå…¨ç›¸åŒçš„åˆ—è¡¨ç»“æ„ï¼Œåªä¿®æ”¹éœ€è¦æ›¿æ¢çš„éƒ¨åˆ†
4. **ç‰¹æ®Šå¤„ç†**ï¼š
   - äº’åŠ¨ç±»æ–‡æœ¬ï¼ˆå¦‚"èµåŒ 117"ã€"æ”¶è—"ï¼‰ä¸ç¿»è¯‘
   - æ ‡ç­¾ç±»æ–‡æœ¬ï¼ˆå¦‚"å…³æ³¨"ã€"çƒ­æ¦œ"ï¼‰ä¸ç¿»è¯‘
   - é•¿æ®µè½åªæ›¿æ¢3-5ä¸ªå…³é”®è¯ï¼Œé¿å…è¿‡åº¦æ›¿æ¢
   - ä¿æŒåŸæ–‡æƒ…æ„Ÿè‰²å½©å’Œå£è¯­é£æ ¼

**ç¤ºä¾‹å‚è€ƒï¼š**
ç”¨æˆ·ç­‰çº§ï¼šåˆçº§
è¾“å…¥ï¼š["æˆ‘å–œæ¬¢è¯»ä¹¦", "ä»Šå¤©å¤©æ°”çœŸå¥½"]
è¾“å‡ºï¼š["æˆ‘å–œæ¬¢è¯»books", "ä»Šå¤©weatherçœŸå¥½"]

ç”¨æˆ·ç­‰çº§ï¼šä¸­çº§
è¾“å…¥ï¼š["ä»–ç»å¸¸ç†¬å¤œå·¥ä½œ"]
è¾“å‡ºï¼š["ä»–ç»å¸¸stay up lateå·¥ä½œ"]

ç”¨æˆ·ç­‰çº§ï¼šé«˜çº§
è¾“å…¥ï¼š["è¿™ä¸ªè§‚ç‚¹å¾ˆæœ‰æ´å¯ŸåŠ›"]
è¾“å‡ºï¼š["è¿™ä¸ªperspectiveå¾ˆæœ‰insight"]

**å½“å‰ä»»åŠ¡ï¼š**
ç”¨æˆ·ç­‰çº§ï¼š{user_level}


è¯·è¿”å›å¤„ç†åçš„å®Œæ•´åˆ—è¡¨ï¼Œä¿æŒåŸæœ‰é¡ºåºå’Œé•¿åº¦ã€‚åªè¾“å‡ºJSONæ ¼å¼çš„åˆ—è¡¨ï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚
"""

            # âœ… ä½¿ç”¨ asyncio.wait_for æ·»åŠ è¶…æ—¶
            completion = await asyncio.wait_for(
                asyncio.to_thread(
                    self.client.chat.completions.create,
                    model="qwen-plus",
                    messages=[
                        {"role": "system", "content": my_prompts},
                        {"role": "user", "content": json.dumps(texts, ensure_ascii=False)},
                    ]
                ),
                timeout=timeout
            )

            # âœ… è§£æå“åº”
            translated_texts = self._parse_response(completion)

            # âœ… è®°å½• API è°ƒç”¨æˆåŠŸ
            elapsed_ms = int((time.time() - start_time) * 1000)
            logger.info("LLM API call successful", extra={
                "model": "qwen-plus",
                "text_count": len(texts),
                "tokens_used": completion.usage.total_tokens if hasattr(completion, 'usage') else None,
                "latency_ms": elapsed_ms
            })

            return translated_texts

        except asyncio.TimeoutError:
            # âœ… è¶…æ—¶é”™è¯¯
            elapsed = int((time.time() - start_time) * 1000)
            logger.error(f"LLM API timeout after {elapsed}ms (limit: {timeout}s)")
            raise LLMAPIException(f"ç¿»è¯‘è¯·æ±‚è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰")

        except json.JSONDecodeError as e:
            # âœ… JSON è§£æé”™è¯¯
            logger.error(f"Failed to parse LLM response: {e}")
            raise LLMAPIException("LLM è¿”å›æ ¼å¼é”™è¯¯")

        except Exception as e:
            # âœ… å…¶ä»–é”™è¯¯
            error_msg = str(e)
            logger.error(f"LLM API call failed: {error_msg}", exc_info=True)

            # æ ¹æ®é”™è¯¯ç±»å‹æä¾›å‹å¥½æ¶ˆæ¯
            if "authentication" in error_msg.lower():
                raise LLMAPIException("API è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¯†é’¥é…ç½®")
            elif "rate_limit" in error_msg.lower():
                raise LLMAPIException("API è°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•")
            else:
                raise LLMAPIException(f"LLM API è°ƒç”¨å¤±è´¥: {error_msg}")

    def _parse_response(self, completion) -> List[str]:
        """
        è§£æ LLM å“åº”

        Args:
            completion: LLM APIå“åº”å¯¹è±¡

        Returns:
            ç¿»è¯‘åçš„æ–‡æœ¬åˆ—è¡¨

        Raises:
            LLMAPIException: å“åº”æ ¼å¼é”™è¯¯æ—¶
        """
        try:
            content = completion.choices[0].message.content
            translated_texts = json.loads(content)

            if not isinstance(translated_texts, list):
                raise LLMAPIException("LLM è¿”å›æ ¼å¼é”™è¯¯: æœŸæœ›æ•°ç»„")

            return translated_texts

        except (IndexError, AttributeError, KeyError) as e:
            logger.error(f"Invalid completion structure: {e}")
            raise LLMAPIException("LLM å“åº”ç»“æ„å¼‚å¸¸")


# æµ‹è¯•ä»£ç  - ä½¿ç”¨ async with
async def test_with_context():
    print("ğŸš€ ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨æµ‹è¯•...")

    # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    async with LLMClient(api_key="dummy_key") as client:
        await client.translate_batch(
            texts=["æµ‹è¯•æ–‡æœ¬1", "æµ‹è¯•æ–‡æœ¬2"],
            user_level="é«˜çº§"
        )

    print("âœ… æµ‹è¯•å®Œæˆï¼ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¼šè‡ªåŠ¨å…³é—­session")


# æµ‹è¯•
if __name__ == "__main__":
    asyncio.run(test_with_context())
