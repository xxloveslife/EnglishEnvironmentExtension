/**
 * Text Planner - Translation Plan Generator
 *
 * Coordinates all optimization modules to create translation plans
 * Implements three-layer deduplication:
 * 1. Node Fingerprint (WeakMap) - same node, same content
 * 2. Text Cache (LRU) - same text across different nodes
 * 3. Batch Deduplication - unique texts within same batch
 */

import type {
  TextNodeData,
  TextCandidate,
  TranslationPlan,
  TranslationPlanStats
} from '../utils/types'
import { TranslationCache } from './TranslationCache'
import { NodeFingerprintStore } from '../utils/NodeFingerprintStore'
import { hashText, generateCacheKey } from '../utils/HashUtils'
import { normalizeText, needsNormalization } from '../utils/TextNormalizer'
import { logger } from '../utils/logger'

export class TextPlanner {
  private fingerprintStore: NodeFingerprintStore

  constructor(fingerprintStore: NodeFingerprintStore) {
    this.fingerprintStore = fingerprintStore
  }

  /**
   * Create translation plan from text nodes
   * Implements three-layer deduplication strategy
   *
   * @param textNodes - Array of text node data from TextExtractor
   * @param cache - Translation cache instance
   * @param userLevel - User proficiency level (A1-C2)
   * @returns Complete translation plan
   *
   * @example
   * const plan = planner.createTranslationPlan(textNodes, cache, 'A1')
   * console.log(plan.stats) // View statistics
   */
  createTranslationPlan(
    textNodes: TextNodeData[],
    cache: TranslationCache,
    userLevel: string
  ): TranslationPlan {
    const startTime = performance.now()

    logger.group('ğŸ“‹ åˆ›å»ºç¿»è¯‘è®¡åˆ’')

    // Initialize stats
    const stats: TranslationPlanStats = {
      total: textNodes.length,
      fingerprintFiltered: 0,
      cached: 0,
      toSend: 0,
      unique: 0,
      normalized: 0,
      batchDedupSaved: 0
    }

    // Step 1: Filter by node fingerprint (first layer deduplication)
    logger.info('Step 1: èŠ‚ç‚¹æŒ‡çº¹è¿‡æ»¤')
    const unfingerprintedNodes = this.fingerprintStore.filterUnprocessed(
      textNodes.map(data => data.node),
      userLevel,
      'original'
    )

    stats.fingerprintFiltered = textNodes.length - unfingerprintedNodes.length

    if (stats.fingerprintFiltered > 0) {
      logger.info(`æŒ‡çº¹è¿‡æ»¤ï¼š${textNodes.length} â†’ ${unfingerprintedNodes.length}`)
    }

    // Build nodeData map for quick lookup
    const nodeDataMap = new Map<Text, TextNodeData>()
    for (const data of textNodes) {
      nodeDataMap.set(data.node, data)
    }

    // Step 2: Create candidates with normalization and hashing
    logger.info('Step 2: è§„èŒƒåŒ–ã€å“ˆå¸Œè®¡ç®—ã€ç¼“å­˜æ£€æŸ¥')
    const candidates: TextCandidate[] = []
    const toApplyFromCache: Array<{ candidate: TextCandidate; translated: string }> = []
    const toSend: TextCandidate[] = []

    for (const node of unfingerprintedNodes) {
      const nodeData = nodeDataMap.get(node)
      if (!nodeData) continue

      const rawText = nodeData.text
      const normalizedText = normalizeText(rawText)
      const textHash = hashText(normalizedText)
      const cacheKey = generateCacheKey(textHash, userLevel)

      // Track normalization effect
      if (needsNormalization(rawText)) {
        stats.normalized++
      }

      const candidate: TextCandidate = {
        nodeData,
        node,
        element: nodeData.element,
        rawText,
        normalizedText,
        textHash,
        cacheKey
      }

      candidates.push(candidate)

      // Check cache (second layer deduplication)
      const cached = cache.getByHash(textHash, userLevel)

      if (cached !== null && cached.length > 0) {
        toApplyFromCache.push({ candidate, translated: cached })
        stats.cached++
        logger.debug(`ç¼“å­˜å‘½ä¸­: "${normalizedText.substring(0, 20)}..." -> "${cached.substring(0, 20)}..."`)
      } else {
        toSend.push(candidate)
      }
    }

    stats.toSend = toSend.length

    logger.info(`éœ€è¦ API å¤„ç†: ${stats.toSend} ä¸ª`)

    // Step 3: Batch deduplication (third layer)
    logger.info('Step 3: æ‰¹æ¬¡å»é‡')
    const { uniqueTexts, textHashMap } = this.batchDeduplicate(toSend)

    stats.unique = uniqueTexts.length
    stats.batchDedupSaved = stats.toSend - stats.unique

    if (stats.batchDedupSaved > 0) {
      const savedPercentage = Math.round((stats.batchDedupSaved / stats.toSend) * 100)
      logger.info(`æ‰¹æ¬¡å»é‡èŠ‚çœ: ${stats.batchDedupSaved} ä¸ªè¯·æ±‚ (${savedPercentage}%)`)
    }

    // Build translation plan
    const plan: TranslationPlan = {
      toApplyFromCache,
      toSend,
      uniqueTexts,
      textHashMap,
      stats
    }

    const elapsed = performance.now() - startTime

    // Log statistics
    logger.info('ç¿»è¯‘è®¡åˆ’ç»Ÿè®¡:')
    logger.info(`  - æ€»èŠ‚ç‚¹æ•°: ${stats.total}`)
    logger.info(`  - æŒ‡çº¹è¿‡æ»¤: ${stats.fingerprintFiltered}`)
    logger.info(`  - ç¼“å­˜å‘½ä¸­: ${stats.cached}`)
    logger.info(`  - éœ€è¦ API: ${stats.toSend}`)
    logger.info(`  - å»é‡åå”¯ä¸€æ–‡æœ¬: ${stats.unique}`)
    logger.info(`  - è§„èŒƒåŒ–å½±å“: ${stats.normalized}`)

    logger.info(`Plan å®Œæˆ (${elapsed.toFixed(2)}ms)`)
    logger.groupEnd()

    return plan
  }

  /**
   * Batch deduplicate texts for API call
   * Groups candidates by text hash
   *
   * @param candidates - Candidates to send to API
   * @returns Unique texts and hash mapping
   */
  private batchDeduplicate(candidates: TextCandidate[]): {
    uniqueTexts: string[]
    textHashMap: Map<string, TextCandidate[]>
  } {
    const textHashMap = new Map<string, TextCandidate[]>()
    const seenHashes = new Set<string>()
    const uniqueTexts: string[] = []

    for (const candidate of candidates) {
      const hash = candidate.textHash

      // Add to hash map (for result mapping)
      if (!textHashMap.has(hash)) {
        textHashMap.set(hash, [])
      }
      textHashMap.get(hash)!.push(candidate)

      // Add to unique texts (for API call)
      if (!seenHashes.has(hash)) {
        seenHashes.add(hash)
        uniqueTexts.push(candidate.normalizedText)
      }
    }

    return { uniqueTexts, textHashMap }
  }

  /**
   * Apply cached translations immediately
   * Updates DOM and marks nodes as processed
   *
   * @param plan - Translation plan
   * @returns Number of translations applied
   *
   * @example
   * const applied = planner.applyCachedTranslations(plan)
   * logger.info(`Applied ${applied} cached translations`)
   */
  applyCachedTranslations(plan: TranslationPlan): number {
    if (plan.toApplyFromCache.length === 0) {
      return 0
    }

    const startTime = performance.now()
    let applied = 0

    logger.group('ğŸ¯ åº”ç”¨ç¼“å­˜ç¿»è¯‘')

    for (const { candidate, translated } of plan.toApplyFromCache) {
      const node = candidate.node

      // Check if node is still in DOM
      if (!node.isConnected || !node.parentElement?.isConnected) {
        logger.warn(`èŠ‚ç‚¹å·²ä» DOM ç§»é™¤ï¼Œè·³è¿‡: "${candidate.normalizedText.substring(0, 20)}..."`)
        continue
      }

      // Apply translation
      node.textContent = translated

      // Mark as processed with 'translated' mode
      this.fingerprintStore.markAsProcessed(node, this.extractUserLevel(candidate.cacheKey), 'translated')

      applied++
    }

    const elapsed = performance.now() - startTime

    logger.info(`æˆåŠŸåº”ç”¨ ${applied}/${plan.toApplyFromCache.length} ä¸ªç¼“å­˜ç¿»è¯‘`)
    logger.info(`Apply å®Œæˆ (${elapsed.toFixed(2)}ms)`)
    logger.groupEnd()

    return applied
  }

  /**
   * Merge API results back to candidates
   * Creates hash-to-translation mapping for DOM application
   *
   * @param plan - Translation plan
   * @param apiResults - Results from API (aligned with plan.uniqueTexts)
   * @returns Map of text hash to translated text
   *
   * @example
   * const hashToTranslation = planner.mergeApiResults(plan, apiResults)
   * // Use hashToTranslation with DomReplacer
   */
  mergeApiResults(
    plan: TranslationPlan,
    apiResults: string[]
  ): Map<string, string> {
    const startTime = performance.now()

    logger.group('ğŸ”„ åˆå¹¶ API ç»“æœ')

    if (apiResults.length !== plan.uniqueTexts.length) {
      logger.error(
        `API ç»“æœé•¿åº¦ä¸åŒ¹é…: expected ${plan.uniqueTexts.length}, got ${apiResults.length}`
      )
      throw new Error('API results length mismatch with unique texts')
    }

    const hashToTranslation = new Map<string, string>()

    // Build normalized text to translation map
    const textToTranslation = new Map<string, string>()
    for (let i = 0; i < plan.uniqueTexts.length; i++) {
      textToTranslation.set(plan.uniqueTexts[i], apiResults[i])
    }

    // Map translations to all candidates (handles duplicates)
    let mapped = 0
    for (const [hash, candidates] of plan.textHashMap) {
      const normalizedText = candidates[0].normalizedText
      const translation = textToTranslation.get(normalizedText)

      if (translation && translation.length > 0) {
        hashToTranslation.set(hash, translation)
        mapped += candidates.length
        logger.debug(`æ˜ å°„ç¿»è¯‘: hash=${hash.substring(0, 8)}... -> ${candidates.length} ä¸ªå€™é€‰é¡¹`)
      } else {
        logger.warn(`API è¿”å›ç©ºç¿»è¯‘: "${normalizedText.substring(0, 20)}..."`)
      }
    }

    const elapsed = performance.now() - startTime

    logger.info(`åˆå¹¶ ${hashToTranslation.size} ä¸ªç¿»è¯‘ç»“æœ`)
    logger.info(`æ˜ å°„åˆ° ${mapped} ä¸ªå€™é€‰é¡¹`)
    logger.info(`Merge å®Œæˆ (${elapsed.toFixed(2)}ms)`)
    logger.groupEnd()

    return hashToTranslation
  }

  /**
   * Extract user level from cache key
   * Cache key format: "textHash:userLevel"
   */
  private extractUserLevel(cacheKey: string): string {
    const parts = cacheKey.split(':')
    return parts[1] || 'A1' // Default to A1 if not found
  }

  /**
   * Get statistics summary string
   *
   * @param plan - Translation plan
   * @returns Formatted statistics string
   */
  getStatsSummary(plan: TranslationPlan): string {
    const { stats } = plan
    const lines = [
      `Total: ${stats.total}`,
      `Fingerprint Filtered: ${stats.fingerprintFiltered}`,
      `Cached: ${stats.cached}`,
      `To Send: ${stats.toSend}`,
      `Unique: ${stats.unique}`,
      `Normalized: ${stats.normalized}`,
      `Batch Dedup Saved: ${stats.batchDedupSaved}`
    ]
    return lines.join(', ')
  }

  /**
   * Validate translation plan integrity
   * Useful for debugging
   *
   * @param plan - Translation plan to validate
   * @returns true if plan is valid
   */
  validatePlan(plan: TranslationPlan): boolean {
    const { stats, toApplyFromCache, toSend, uniqueTexts, textHashMap } = plan

    // Check stats consistency
    const expectedTotal = stats.fingerprintFiltered + toApplyFromCache.length + toSend.length
    if (stats.total !== expectedTotal) {
      logger.error(
        `Plan validation failed: total mismatch (${stats.total} !== ${expectedTotal})`
      )
      return false
    }

    // Check unique texts count
    if (uniqueTexts.length !== stats.unique) {
      logger.error(
        `Plan validation failed: unique texts count mismatch (${uniqueTexts.length} !== ${stats.unique})`
      )
      return false
    }

    // Check hash map size
    if (textHashMap.size !== uniqueTexts.length) {
      logger.error(
        `Plan validation failed: hash map size mismatch (${textHashMap.size} !== ${uniqueTexts.length})`
      )
      return false
    }

    logger.debug('Plan validation passed')
    return true
  }
}
